{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554304bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) Generate sequence of length k using Student model\n",
    "# 2.) Evaluate drafted sequence using Teacher model\n",
    "# 3.) Evaluate drafted sequence using Student model\n",
    "# 4.) Calculate loss\n",
    "# 5.) Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0268f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model_id = \"Qwen/Qwen3-0.6B\"\n",
    "# teacher_model_id = \"Qwen/Qwen3-1.7B\"\n",
    "student_model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "teacher_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(student_model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    student_model_id,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:1\",\n",
    ")\n",
    "student.config.pad_token_id = pad_token_id\n",
    "student.config.eos_token_id = eos_token_id\n",
    "\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_model_id,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:1\",\n",
    ").eval()\n",
    "teacher.config.pad_token_id = pad_token_id\n",
    "teacher.config.eos_token_id = eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a96b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================\n",
    "# Need to find a way to tokenize text that preserves the attention mask while also\n",
    "# making a clear END to the user input. Currently, the models are just completing\n",
    "# whatever the input text was if the tokenizer truncates.\n",
    "# ================\n",
    "def tokenize_function(examples, tokenizer, max_length):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, factual assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Summarize this in 1 sentence {examples['article']}\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    end_marker = \"\\n\\nSummary:\"\n",
    "    end_ids = tokenizer(end_marker, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    tokenized_prompt = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        enable_thinking=False,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length - len(end_ids),\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized_prompt[\"input_ids\"] + end_ids,\n",
    "        # \"attention_mask\": tokenized_prompt[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_dataset = ds.map(\n",
    "    tokenize_function,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer, \"max_length\": 256},\n",
    "    batched=False,\n",
    "    remove_columns=ds.column_names,\n",
    ")\n",
    "\n",
    "# 2) Tell datasets to return torch tensors on indexing\n",
    "tokenized_dataset = tokenized_dataset.with_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede4166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use this for training loop\n",
    "dl = DataLoader(tokenized_dataset, batch_size=1, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0ed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate sequence with length k from Student model\n",
    "k = 64\n",
    "\n",
    "# Sample per iteration\n",
    "# torch.distributions.uniform.Uniform(0.75, 1.25).sample()\n",
    "\n",
    "for batch in dl:\n",
    "    with torch.no_grad():\n",
    "        outputs = student.generate(\n",
    "            input_ids=batch[\"input_ids\"].to(student.device),\n",
    "            # attention_mask=batch[\"attention_mask\"].to(student.device),\n",
    "            max_new_tokens=k,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03fbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer.decode(outputs[0][: batch[\"input_ids\"].shape[1]]))\n",
    "print(tokenizer.decode(outputs[0][batch[\"input_ids\"].shape[1] :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf4bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.) Evaluate drafted sequence using Teacher model\n",
    "# Use up to k-1, teacher and student will predict next token (we already have the label)\n",
    "student_draft = outputs[:, :-1]\n",
    "with torch.no_grad():\n",
    "    logits_t = teacher(student_draft).logits\n",
    "    logits_t = logits_t[:, batch[\"input_ids\"].shape[1] :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.) Evaluate drafted sequence using Student model\n",
    "logits_s = student(student_draft).logits\n",
    "logits_s = logits_s[:, batch[\"input_ids\"].shape[1] :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a3f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.) Calculate loss\n",
    "T = 1.0\n",
    "logp_s = torch.log_softmax(logits_s / T, dim=-1)\n",
    "p_t = torch.softmax(logits_t / T, dim=-1)\n",
    "kd_tok = torch.nn.functional.kl_div(logp_s, p_t, reduction=\"none\") * (T * T)\n",
    "kd_seq = kd_tok.sum(dim=-1)\n",
    "loss = kd_seq.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae95fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.) Backprop\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "topN_t, topN_idx = torch.topk(logits_t.cpu(), N)\n",
    "topN_s = torch.gather(logits_s.detach().cpu(), dim=2, index=topN_idx)\n",
    "\n",
    "plt.title(f\"Teacher vs. Student (top {N} logits, teacher ranked)\")\n",
    "plt.plot(topN_t[0, 0, :], \"-o\", label=\"Teacher\")\n",
    "plt.plot(topN_s[0, 0, :], \"-o\", label=\"Student\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.title(f\"Teacher vs. Student (top {N} probabilities)\")\n",
    "plt.plot(torch.softmax(topN_t[0, 0, :], dim=-1), \"-o\", label=\"Teacher\")\n",
    "plt.plot(torch.softmax(topN_s[0, 0, :], dim=-1), \"-o\", label=\"Student\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796abd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Draft sequence\")\n",
    "print(tokenizer.decode(outputs[0][batch[\"input_ids\"].shape[1] :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Student Output (greedy decoding)\")\n",
    "print(tokenizer.decode(logits_s.argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Teacher Output (greedy decoding)\")\n",
    "print(tokenizer.decode(logits_t.argmax(dim=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbccb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "specalign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
